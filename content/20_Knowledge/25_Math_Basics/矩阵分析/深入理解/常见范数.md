---
publish: true
title:
---
在矩阵分析和数值计算中，范数（Norm）是衡量向量“长度”或矩阵“大小”的度量。为了方便记忆和对比，我将最常见的范数整理为向量和矩阵两个部分。

### 一、 向量范数 (Vector Norms)
设向量 $x = [x_1, x_2, \dots, x_n]^T \in \mathbb{C}^n$。

| 范数名称 | 符号定义 | 计算公式 | 直观理解 / 物理意义 |
| :--- | :--- | :--- | :--- |
| **1-范数** | $\|x\|_1$ | $\sum_{i=1}^n |x_i|$ | **曼哈顿距离**：所有坐标绝对值之和。 |
| **2-范数** | $\|x\|_2$ | $\sqrt{\sum_{i=1}^n |x_i|^2}$ | **欧几里得长度**：平常物理意义上的直线距离。 |
| **$\infty$-范数** | $\|x\|_\infty$ | $\max_{1 \le i \le n} |x_i|$ | **切比雪夫距离**：所有坐标中绝对值最大的那一个。 |
| **$p$-范数** | $\|x\|_p$ | $(\sum_{i=1}^n |x_i|^p)^{1/p}$ | 通用定义（$p \ge 1$）。 |
| **0-范数** | $\|x\|_0$ | 非零元素的个数 | 非严格意义范数，常用于**稀疏表示**。 |

---

### 二、 矩阵范数 (Matrix Norms)
矩阵范数分为两类：一类是由向量范数**诱导**出来的（Induced Norms），另一类是把矩阵看作长向量计算的**元素级**范数。

设矩阵 $A = (a_{ij}) \in \mathbb{C}^{m \times n}$，其奇异值为 $\sigma_1 \ge \sigma_2 \ge \dots \ge \sigma_r > 0$。

#### 1. 诱导范数（算子范数）
这组范数的定义逻辑是：$\|A\| = \max_{x \neq 0} \frac{\|Ax\|}{\|x\|}$，即矩阵 $A$ 对向量的最大放大倍数。

| 范数名称 | 符号定义 | **记忆快捷计算法** | 特点 / 别名 |
| :--- | :--- | :--- | :--- |
| **1-范数** | $\|A\|_1$ | **列模最大和**：$\max_j \sum_i |a_{ij}|$ | 每一列元素绝对值求和，取最大的那一列。 |
| **$\infty$-范数** | $\|A\|_\infty$ | **行模最大和**：$\max_i \sum_j |a_{ij}|$ | 每一行元素绝对值求和，取最大的那一行。 |
| **2-范数** | $\|A\|_2$ | **最大奇异值**：$\sigma_{\max}(A)$ | 别名 **谱范数 (Spectral Norm)**。最常用。 |

#### 2. 非诱导范数（元素级及其他）

| 范数名称 | 符号定义 | 计算公式 | 特点 / 关系 |
| :--- | :--- | :--- | :--- |
| **F-范数** | $\|A\|_F$ | $\sqrt{\sum_{i,j} |a_{ij}|^2} = \sqrt{\text{tr}(A^H A)}$ | **Frobenius 范数**：所有元素平方和再开方。 |
| **核范数** | $\|A\|_*$ | $\sum \sigma_i$ | **奇异值之和**：常用于机器学习中的低秩约束。 |
| **L1,1范数** | $\|A\|_{1,1}$ | $\sum_{i,j} |a_{ij}|$ | 简单把矩阵拆散求和。 |

---

### 三、 核心结论与常用不等式（避坑指南）

在做矩阵分析题时，以下几点非常重要：

1.  **F-范数 vs 2-范数**：
    *   $\|A\|_F = \sqrt{\sigma_1^2 + \dots + \sigma_r^2}$
    *   $\|A\|_2 = \sigma_1$
    *   显然有：$\|A\|_2 \le \|A\|_F \le \sqrt{\text{rank}(A)} \|A\|_2$。

2.  **相容性 (Compatibility)**：
    矩阵范数通常需要满足 $\|Ax\| \le \|A\| \|x\|$。诱导范数天然满足这一点。

3.  **矩阵乘法的相容性 (Sub-multiplicativity)**：
    对于方阵，通常要求满足 $\|AB\| \le \|A\| \|B\|$。**注意：** 并不是所有定义的矩阵范数都满足这一条，但上述 1, 2, $\infty$, F 范数均满足。

4.  **如何快速记忆？**
    *   **1** 是列（**1** 像一根柱子 $\to$ 列）。
    *   **$\infty$** 是行（$\infty$ 是横着的 $\to$ 行）。
    *   **2** 是谱（2 与奇异值/特征值相关）。

### 四、 总结：什么时候用哪个？
*   **误差分析/收敛性**：多用 $1, 2, \infty$ 诱导范数，因为它们与向量误差直接挂钩。
*   **最优化/机器学习**：多用 **F-范数**（容易求导）或 **核范数**（用于降维和稀疏）。
*   **理论证明**：**2-范数**最强大，因为它与矩阵的奇异值结构紧密相连。